<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="_2qF0otnmEdmO6L4XMImrsA" name="workload_analysis_model,2.7672246217039647E-305" guid="_2qF0otnmEdmO6L4XMImrsA" changeDate="2005-06-21T11:42:59.458-0700" version="7.1.0">
  <mainDescription>&lt;h3>&#xD;
    Overview&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Software quality is assessed along different dimensions, including reliability, function, and performance (see &lt;a&#xD;
    class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/guidances/concepts/quality_dimensions_19CC5BD2.html&quot;&#xD;
    guid=&quot;1.0990958932232138E-305&quot;>Concept: Quality Dimensions&lt;/a>). The Workload Analysis Model (see &lt;a&#xD;
    class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/workproducts/rup_workload_analysis_model_E86A32FF.html&quot;&#xD;
    guid=&quot;{008164B1-3AC3-47E8-9F93-EE4C36B58A97}&quot;>Artifact: Workload Analysis Model&lt;/a>) is created to identify and define&#xD;
    the different variables that affect or influence an application or system's performance and the measures required to&#xD;
    assess performance. The workload profiles that make up the model represent candidates for conditions to be simulated&#xD;
    against the Target Test Items under one or more Test Environment Configurations. The workload analysis model is used by&#xD;
    the following roles:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        the &lt;b>test analyst&lt;/b> (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/roles/rup_test_analyst_4637F9F0.html&quot; guid=&quot;{8728060F-9DAD-42AD-B0B6-668C9AEA531D}&quot;>Role: Test&#xD;
        Analyst&lt;/a>) uses the workload analysis model to identify test ideas and define test cases for different tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>test designer&lt;/b> (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/roles/rup_test_designer_5F59E64F.html&quot; guid=&quot;{84F723B5-288F-4AC4-B6C1-C75A07BFEEED}&quot;>Role:&#xD;
        Test Designer&lt;/a>) uses the workload analysis model to define an appropriate test approach and identify testability&#xD;
        needs for the different tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>tester&lt;/b> (see &lt;a class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/roles/rup_tester_8ADFF3FA.html&quot;&#xD;
        guid=&quot;{A719CEAE-35FB-42F6-A076-F501B83E5B85}&quot;>Role: Tester&lt;/a>) uses the workload analysis model to better&#xD;
        understand the goals of the test to implement, execute and analyze its execution properly&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        the &lt;b>user representative&lt;/b> (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/roles/rup_stakeholder_1011D446.html&quot; guid=&quot;{7A4262EA-AB5B-431E-BB4A-3822FBF7F239}&quot;>Role:&#xD;
        Stakeholder&lt;/a>) uses the workload analysis model to assess the appropriateness of the workload, and the tests&#xD;
        required to effectively assess the systems behavior against that workload analysis model&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    The information included in the workload analysis model focuses on characteristics and attributes in the following&#xD;
    primary areas:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Use-Case Scenarios (or Instances, see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/workproducts/rup_usecase_A5D30E62.html&quot;&#xD;
        guid=&quot;{B1526BC5-E346-42CB-A08A-3C0D7F382407}&quot;>Artifact: Use Case&lt;/a>) to be executed and evaluated during the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Actors (see &lt;a class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/workproducts/rup_actor_1F9CE9.html&quot;&#xD;
        guid=&quot;{0EEBBA43-F5C6-4594-9F06-65D99F7556DF}&quot;>Artifact: Actor&lt;/a>) to be simulated / emulated during the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Workload profile - representing the number and type of simultaneous actor instances, use-case scenarios executed by&#xD;
        those actor instances, and on-line responses or throughput associated with each use-case scenario.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Test Environment Configuration (actual, simulated or emulated) to be used in executing and evaluating the tests&#xD;
        (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/workproducts/rup_test_environment_configuration_B6BBDD6.html&quot;&#xD;
        guid=&quot;{DB50EF75-2B05-4487-8299-D82D2A0AD45C}&quot;>Artifact: Test Environment Configuration&lt;/a>. Also see &lt;a&#xD;
        class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/workproducts/rup_software_architecture_document_C367485C.html&quot;&#xD;
        guid=&quot;{6F49641A-ED10-47B5-9E5D-3F90A6BF3006}&quot;>Artifact: Software Architecture Document&lt;/a>, Deployment view, which&#xD;
        should form the basis for the Test Environment Configuration)&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Tests should be considered to measure and evaluate the characteristics and behaviors of the target-of-test when&#xD;
    functioning under different workloads. Successfully designing, implementing, and executing these tests requires&#xD;
    identifying both realistic and exceptional data for these workload profiles.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Use Cases and Use Case Attributes&quot; name=&quot;Use Cases and Use Case Attributes&quot;>Use Cases and Use Case&#xD;
    Attributes&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Two aspects of use cases are considered for selection of scenarios for this type of testing:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Critical%20Use%20Cases&quot;>critical use cases&lt;/a> contain the key use-case scenarios to be measured and&#xD;
        evaluated in the tests&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;a href=&quot;#Significant%20Use%20Cases&quot;>significant use cases&lt;/a> contain use-case scenarios that may impact the&#xD;
        behavior of the critical use-case scenarios&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Critical Use Cases&quot; name=&quot;Critical Use Cases&quot;>Critical Use Cases&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Not all use-case scenarios being implemented in the target-of-test may be needed for these tests. Critical use cases&#xD;
    contain those use-case scenarios that will be the focus of the test - that is their behaviors will be measured and&#xD;
    evaluated.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    To identify the critical use cases, identify those use-case scenarios that meet one or more of the following criteria:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        require measurement and assessment based on workload profile&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        are executed frequently by one or more end-users (actor instances)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        that represent a high percentage of system use&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        that consume significant system resources&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    List the critical use-case scanners for inclusion in the test. As theses are being identified, the use case flow of&#xD;
    events should be reviewed. Begin to identify the specific sequence of events between the actor (type) and system when&#xD;
    the use-case scenario is executed.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Additionally, identify (or verify) the following information:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Preconditions for the use cases, such as the state of the data (what data should / should not exist) and the state&#xD;
        of the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Data that may be constant (the same) or must differ from one use-case scenario to the next&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Relationship between the use case and other use cases, such as the sequence in which the use cases must be&#xD;
        performed.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The frequency of execution of the use-case scenario, including characteristics such as the number of simultaneous&#xD;
        instances of the use case and the percent of the total load each scenario places on the system.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    &lt;a id=&quot;Significant Use Cases&quot; name=&quot;Significant Use Cases&quot;>Significant Use Cases&lt;/a>&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Unlike critical use-case scenarios, which are the primary focus of the test, significant use-case scenarios are those&#xD;
    that may impact the performance behaviors of critical use-case scenarios. Significant use-case scenarios include those&#xD;
    that meet one or more of the following criteria:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        they must be executed before or after executing a critical use case (a dependent precondition or postcondition)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they are executed frequently by one or more actor instances&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they represent a high percentage of system use&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they require significant system resources&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        they will be executed routinely on the deployed system while critical use-case scenarios are executed, such as&#xD;
        e-mail or background printing&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    As the significant use-case scenarios are being identified and listed, review the use case flow of events and&#xD;
    additional information as done above for the critical use-case scenarios.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Actors and Actor Attributes&quot; name=&quot;Actors and Actor Attributes&quot;>Actors and Actor Attributes&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Successful performance tests requires identifying not just the actors executing the critical and significant use-case&#xD;
    scenarios, but must also simulate / emulate actor behavior. That is, one instance of an actor may interact with the&#xD;
    target-of-test differently (take longer to respond to prompts, enter different data values, etc.) while executing the&#xD;
    same use-case scenario as another instance of that actor. Consider the simple use cases below:&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;226&quot; alt=&quot;Diagram described in caption.&quot; src=&quot;resources/tstcs002.gif&quot; width=&quot;354&quot; />&#xD;
&lt;/p>&#xD;
&lt;p class=&quot;picturetext&quot;>&#xD;
    Actors and use cases in an ATM machine.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    The first instance of the &quot;Customer&quot; actor executing a use-case scenario might be an experienced ATM user, while&#xD;
    another instance of the &quot;Customer&quot; actor may be inexperienced at ATM use. The experienced Customer quickly navigates&#xD;
    through the ATM user-interface and spends little time reading each prompt, instead, pressing the buttons by rote. The&#xD;
    inexperienced Customer however, reads each prompt and takes extra time to interpret the information before responding.&#xD;
    Realistic workload profiles reflect this difference to ensure accurate assessment of the behaviors of the&#xD;
    target-of-test.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Begin by identifying the actors for each use-case scenario identified above. Then identify the different actor profiles&#xD;
    that may execute each use-case scenario. In the ATM example above, we may have the following actor stereotypes:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Experienced ATM user&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Inexperienced ATM user&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        ATM user's account is &quot;inside&quot; the ATM's bank network (user's account is with bank owning ATM)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        ATM user's account is outside the ATM's bank network (competing bank)&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    For each actor profile, identify the different attributes and their values such as:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Think time - the period of time it takes for an actor to respond to a target-of-test's individual prompts&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Typing rate - the rate at which the actor interacts with the interface&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Request Pace - the rate at which the actor makes requests of the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Repeat factor - the number of times a use case or request is repeated in sequence&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Interaction method - the method of interaction used by the actor, such as using the keyboard to enter in values,&#xD;
        tabbing to a field, using accelerator keys, etc., or using the mouse to &quot;point and click&quot;, &quot;cut and paste&quot;, etc.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Additionally, for each actor profile identify their workload profile, specifying all the use-case scenarios they&#xD;
    execute, and the percentage of time or proportion of effort spent by the actor executing these scenarios. Identifying&#xD;
    this information is used in identifying and creating a realistic load (see Load and Load Attributes below).&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;System Attributes and Variables&quot; name=&quot;System Attributes and Variables&quot;>System Attributes and Variables&lt;/a> &lt;a&#xD;
    href=&quot;#Top&quot;>&lt;img height=&quot;20&quot; alt=&quot;To top of page&quot; src=&quot;./../../../core.base_rup/resources/top.gif&quot; width=&quot;26&quot; border=&quot;0&quot; />&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    The specific attributes and variables of the Test Environment Configuration that uniquely identify the environment must&#xD;
    also be identified, as these attributes also impact the measurement and evaluation of behavior. These attributes&#xD;
    include:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        The physical hardware (CPU speed, memory, disk caching, etc.)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The deployment architecture (number of servers, distribution of processing, etc.)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The network attributes&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Other software (and use cases) that may be installed and executed simultaneously to the target-of-test&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Identify and list the system attributes and variables that are to be considered for inclusion in the tests. This&#xD;
    information may be obtained from several sources, including:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        The Software Architecture Document (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/workproducts/rup_software_architecture_document_C367485C.html&quot;&#xD;
        guid=&quot;{6F49641A-ED10-47B5-9E5D-3F90A6BF3006}&quot;>Artifact: Software Architecture Document&lt;/a>, Deployment View)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The Vision Document (see &lt;a class=&quot;elementlinkwithusertext&quot;&#xD;
        href=&quot;./../../../core.base_rup/workproducts/rup_vision_2D6D6F1.html&quot; guid=&quot;{417F089F-6636-451A-A8AB-AB4EAC7AC4F1}&quot;>Work&#xD;
        Product: Vision Document&lt;/a>)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The Stakeholder Requests (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
        href=&quot;./../../../core.base_rup/workproducts/rup_stakeholder_requests_A89D2BF9.html&quot;&#xD;
        guid=&quot;{75F38765-E25B-4459-907E-77F429652C0F}&quot;>Artifact: Stakeholder Requests&lt;/a>)&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Workload Profiles&quot; name=&quot;Workload Profiles&quot;>Workload Profiles&lt;/a> &lt;a href=&quot;#Top&quot;>&lt;img height=&quot;20&quot;&#xD;
    alt=&quot;To top of page&quot; src=&quot;./../../../core.base_rup/resources/top.gif&quot; width=&quot;26&quot; border=&quot;0&quot; />&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    As stated previously, workload is an important factor that impacts the behavior of a target-of-test. Accurately&#xD;
    identifying the workload profile that will be used to evaluate the targets behavior is critical. Typically, test that&#xD;
    involve workload are executed several times using different workload profiles, each representing a variation of the&#xD;
    attributes described below:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        The number of simultaneous actor instances interacting with the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The profile of the actors interacting with the target-of-test&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The use-case scenarios executed by each actor instance&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        The frequency of each critical use-case scenarios executed and how often it is repeated&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    For each workload profile used to evaluate the performance of the target-of-test, identify the values for each of the&#xD;
    above variables. The values used for each variable in the different loads may be derived by observing or interviewing&#xD;
    actors or, from the Business Use-Case Model&amp;nbsp; if one is available. It is common for one or more of the following&#xD;
    workload profiles to be defined:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Optimal - a workload profile that reflects the best possible deployment conditions, such as a minimal number of&#xD;
        actor instances interacting with the system, executing only the critical use-case scenarios, with minimal&#xD;
        additional software and workload executing during the test.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Average (AKA Normal) - a workload profile that reflects the anticipated or actual average usage conditions.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Instantaneous Peak - a workload profile that reflects anticipated or actual instantaneous heavy usage conditions,&#xD;
        that occur for short periods during normal operation.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Peak - a workload profile that reflects anticipated or actual heavy usage conditions, such as a maximum number of&#xD;
        actor instances, executing high volumes of use-case scenarios, with much additional software and workload executing&#xD;
        during the test.&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    When workload testing includes Stress Testing (see &lt;a class=&quot;elementlinkwithusertext&quot;&#xD;
    href=&quot;./../../../core.base_rup/guidances/concepts/performance_testing_37A31809.html&quot; guid=&quot;8.027577129450698E-306&quot;>Concept:&#xD;
    Performance Test&lt;/a> and &lt;a class=&quot;elementlinkwithusertext&quot;&#xD;
    href=&quot;./../../../core.base_rup/guidances/concepts/types_of_test_8AB94831.html&quot; guid=&quot;1.6037730846300355E-307&quot;>Technique: Test&#xD;
    Types&lt;/a>), several additional loads should be identified, each targeting specific aspects of the system in abnormal or&#xD;
    unexpected states beyond the expected normal capacity of the deployed system.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    &lt;a id=&quot;Performance Measurements and Criteria&quot; name=&quot;Performance Measurements and Criteria&quot;>Performance Measurements and&#xD;
    Criteria&lt;/a> &lt;a href=&quot;#Top&quot;>&lt;img height=&quot;20&quot; alt=&quot;To top of page&quot; src=&quot;./../../../core.base_rup/resources/top.gif&quot; width=&quot;26&quot;&#xD;
    border=&quot;0&quot; />&lt;/a>&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Successful workload testing can only be achieved if the tests are measured and the workload behaviors evaluated. In&#xD;
    identifying workload measurements and criteria, the following factors should be considered:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        What measurements are to be made?&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Where / what are the critical measurement points in the target-of-test / use-case execution.&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        What are the criteria to be used for determining acceptable performance behavior?&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;h4>&#xD;
    Performance Measurements&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    There are many different measurements that can be made during test execution. Identify the significant measurements to&#xD;
    be made and justify why they are the most significant measurements.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Listed below are the more common performance behaviors monitored or captured:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        Test script state or status - a graphical depiction of the current state, status, or progress of the test execution&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        Response time / Throughput - measurement (or calculation) of response times or throughput (usually stated as&#xD;
        transactions per second).&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        &lt;font color=&quot;#000000&quot;>Traces - capturing the messages / conversations between the actor (test script) and the&#xD;
        target-of-test, or the dataflow and / or process flow during execution.&lt;/font>&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    See &lt;a class=&quot;elementlinkwithtype&quot; href=&quot;./../../../core.base_rup/guidances/concepts/key_measures_of_test_62253EE4.html&quot;&#xD;
    guid=&quot;5.312818155786224E-305&quot;>Concept: Key Measures of Test&lt;/a> for additional information&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    Critical Performance Measurement Points&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    In the Use Cases and Use Case Attributes section above, it was noted that not all use cases and their scenarios are&#xD;
    executed for performance testing. Similarly, not all performance measures are made for each executed use-case scenario.&#xD;
    Typically only specific use-case scenarios are targeted for measurement, or there may be a specific sequence of events&#xD;
    within a specific use-case scenario that will be measured to assess the performance behavior. Care should be taken to&#xD;
    select the most significant starting and ending &quot;points&quot; for the measuring the performance behaviors. The most&#xD;
    significant ones are typically those the most visible sequences of events or those that we can affect directly through&#xD;
    changes to the software or hardware.&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    For example, in the ATM - Cash Withdraw use case identified above, we may measure the performance characteristics of&#xD;
    the entire use-case instance, from the point where the Actor initiates the withdrawal, to the point in which the use&#xD;
    case is terminated - that is, the Actor receives their bank card and the ATM is now ready to accept another card, as&#xD;
    shown by the black &quot;Total Elapsed Time&quot; line in the diagram below:&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;center&quot;>&#xD;
    &lt;img height=&quot;514&quot; alt=&quot;Diagram is described in the content.&quot; src=&quot;resources/md_wlmd1.gif&quot; width=&quot;384&quot; />&#xD;
&lt;/p>&#xD;
&lt;p align=&quot;left&quot;>&#xD;
    Notice, however, there are many sequences of events that contribute to the total elapsed time, some that we may have&#xD;
    control over (such as read card information, verify card type, initiate communication with bank system, etc., items B,&#xD;
    D, and E above), but other sequences, we have not control over (such as the actor entering their PIN or reading the&#xD;
    prompts before entering their withdrawal amount, items A, C, and F). In the above example, in addition to measuring the&#xD;
    total elapsed time, we would measure the response times for sequences B, D, and E, since these events are the most&#xD;
    visible response times to the actor (and we may affect them via the software / hardware for deployment).&#xD;
&lt;/p>&#xD;
&lt;h4>&#xD;
    Performance Measurement Criteria&#xD;
&lt;/h4>&#xD;
&lt;p>&#xD;
    Once the critical performance measures and measurement points have been identified, review the performance criteria.&#xD;
    Performance criteria are stated in the Supplemental Specifications (see &lt;a class=&quot;elementlinkwithtype&quot;&#xD;
    href=&quot;./../../../core.base_rup/workproducts/rup_supplementary_specification_F5ACAA22.html&quot;&#xD;
    guid=&quot;{B16C2941-791C-44E6-B353-354109B5C9DE}&quot;>Artifact: Supplementary Specifications&lt;/a>). If necessary revise the&#xD;
    criteria.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Here are some criteria that are often used for performance measurement:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        response time (AKA on-line response)&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        throughput rate&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        response percentiles&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    On-line response time, measured in seconds, or transaction throughput rate, measured by the number of transactions (or&#xD;
    messages) processed is the main criteria.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    For example, using the Cash Withdraw use case, the criteria is stated as &quot;events B, D, and E (see diagram above) must&#xD;
    each occur in under 3 seconds (for a combined total of 9 seconds)&quot;. If during testing, we note that that any one of the&#xD;
    events identified as B, D, or E takes longer than the stated 3 second criteria, we would note a failure.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Percentile measurements are combined with the response times and / or throughput rates and are used to &quot;statistically&#xD;
    ignore&quot; measurements that are outside of the stated criteria. For example, the performance criteria for the use case&#xD;
    was now states &quot;for the 90th percentile, events B, D, and E must each occur in under 3 seconds ...&quot;. During test&#xD;
    execution, if we measure 90 percent of all performance measurements occur within the stated criteria, no failures are&#xD;
    noted.&#xD;
&lt;/p>&lt;br />&#xD;
&lt;br /></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
